{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubmed Pre-processing & EDA\n",
    "\n",
    "\n",
    "#### TO DO For scraping script:\n",
    "- check if has abstract\n",
    "- check if title is duplicate\n",
    "- clean titles (remove parenthesis)\n",
    "- clean dates - make sure year is greater than 1997 or not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime,re, string, timeit, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from  sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if want smaller, 2-piece dataset, else the large 64k dataset\n",
    "df1 = False\n",
    "\n",
    "if df1:\n",
    "\n",
    "    df1 = pd.read_csv(\"pubmed_abstracts_batchall_1.csv\")\n",
    "    df2 = pd.read_csv(\"pubmed_abstracts_batchall_2.csv\")\n",
    "\n",
    "    df = pd.concat([df1,df2],ignore_index=True)\n",
    "\n",
    "    print(df.info())\n",
    "else:\n",
    "    df = pd.read_csv(\"pubmed_abstracts_export_all_2018-07-09.csv\")\n",
    "    df.drop(['article_title','date'],axis=1,inplace=True)\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "#### Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset='Article_title')].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Article_title']=='2017 ACC/AHA/AAPA/ABC/ACPM/AGS/APhA/ASH/ASPC/NMA/PCNA Guideline for the Prevention, Detection, Evaluation, and Management of High Blood Pressure in Adults: A Report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Article_title'].isin(df[df.duplicated(subset='Article_title')]['Article_title'])].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 208 duplicate articles (using title as the ID), with 403 total affected. \n",
    "\n",
    "#### Dropping duplicates by Article Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='Article_title',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping articles w/o Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['abstract'].isnull() ==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ISSN'].isnull()==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Article_title'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parentheses(x):\n",
    "    \n",
    "    return x.replace(\"[\",'').replace(']','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Article_title'] = df['Article_title'].apply(lambda x: remove_parentheses(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map = {\n",
    "    'Jan': 1,\n",
    "    'Feb': 2,\n",
    "    'Mar': 3,\n",
    "    'Apr': 4,\n",
    "    'May': 5,\n",
    "    'Jun': 6,\n",
    "    \"Jul\": 7,\n",
    "    'Aug': 8,\n",
    "    'Sep': 9,\n",
    "    'Oct': 10,\n",
    "    'Nov': 11,\n",
    "    'Dec': 12\n",
    "}\n",
    "\n",
    "\n",
    "def clean_month(x):\n",
    "    x = str(x)\n",
    "    date = x.split('/')\n",
    "    year = date[0]\n",
    "    month = date[1]\n",
    "    day = date[2]\n",
    "    \n",
    "    clean_month = None\n",
    "    \n",
    "    if month in month_map.keys():\n",
    "        clean_month = month_map[month]\n",
    "    else:\n",
    "        clean_month = month\n",
    "    \n",
    "    return str(year) + '/' + str(clean_month) + '/' + str(day)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].fillna('1900/01/01',inplace=True)\n",
    "\n",
    "df['Clean_Date'] = pd.to_datetime(df['Date'].apply(lambda x: clean_month(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'] = df['Clean_Date'].apply(str).apply(lambda x: x.split('-')[1])\n",
    "df['Year'] = df['Clean_Date'].apply(str).apply(lambda x: x.split('-')[0])\n",
    "df['Day']  = df['Clean_Date'].apply(str).apply(lambda x: x.split('-')[2][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what years were most of the papers published in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(17,8))\n",
    "\n",
    "#ax.figure(figsize=(18,8))\n",
    "ax.plot(df.groupby(by=[df['Year']])['Article_title'].count()) #+ df['Month']\n",
    "plt.xlim('1999','2018')\n",
    "ax.set_title(\"Number of Papers in dataset by Year\")\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Year')\n",
    "#plt.xlim([datetime.date(1975, 1, 1), datetime.date(2019, 1, 1)])\n",
    "\n",
    "#ax.set_xticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our papers are from the years 2009 on.\n",
    "\n",
    "#### Removing the papers from longer than 20+ years ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Year'].astype(int) > 1997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the words in the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(x,sentences = False):\n",
    "    '''\n",
    "    Function that takes in an abstract, and cleans it.\n",
    "    1. lowers all characters\n",
    "    2. gets rid of words like a or I or as\n",
    "    3. gets root of words using wornet lemmatizer\n",
    "    4. gets words that are not stopwords\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    #turns words into base form, so dogs == dog\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #loading stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    if sentences:\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "        x = x.lower()\n",
    "        \n",
    "        x = x.split(\".\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for sent in x:\n",
    "            sent = regex.sub(' ', sent)\n",
    "            \n",
    "            tokens = nltk.tokenize.word_tokenize(sent)\n",
    "            tokens = [t for t in tokens if len(t) > 2]\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "            clean_tokens = []\n",
    "            for token in tokens:\n",
    "                if token not in stop_words:\n",
    "                    if token.isdigit() == False:\n",
    "                        clean_tokens.append(token)\n",
    "\n",
    "            #clean_tokens = [token for token in clean_tokens if not token.isdigit()]\n",
    "            result = ' '.join(token for token in clean_tokens)\n",
    "            \n",
    "            if result != '':\n",
    "                results.append(result)\n",
    "            \n",
    "        return results   \n",
    "    \n",
    "    else:\n",
    "    \n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "        x = x.lower()\n",
    "        x = regex.sub(' ', x)\n",
    "\n",
    "        tokens = nltk.tokenize.word_tokenize(x)\n",
    "        tokens = [t for t in tokens if len(t) > 2]\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "        clean_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                if token.isdigit() == False:\n",
    "                    clean_tokens.append(token)\n",
    "\n",
    "        #clean_tokens = [token for token in clean_tokens if not token.isdigit()]\n",
    "        result = ' '.join(token for token in clean_tokens)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if want to clean the abstracts before processing\n",
    "df['Clean_Abstract'] = df['abstract'].apply(lambda x: my_tokenizer(x))\n",
    "df = df[df['Clean_Abstract'].isnull() == False]\n",
    "df.to_csv(\"pubmed_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
